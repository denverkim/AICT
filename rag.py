# -*- coding: utf-8 -*-
"""RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B_wWK3wrYH8P_pNc2wjlPFMHQ2Iijixi

# RAG (Retrieval-Augmented Generation) implementation pipeline

# RAG pipeline using OpenAI + Chroma
"""

# 1. Install required libraries
!pip install langchain langchain_community langchain_openai openai faiss-cpu pypdf tiktoken chromadb

# 2. Setup OpenAI API Key
from google.colab import userdata
OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')

# 3. Import libraries
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA

# 4. Load PDF document
loader = PyPDFLoader("/content/(공고문)차세대융합기술연구원 임원(원장,비상임임원) 공개모집 공고(2025-1).pdf")
documents = loader.load()
print(f"Loaded {len(documents)} document chunks")
print(documents[0])

# 5. Split document into chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,      # Maximum chunk size
    chunk_overlap=50     # Overlap between chunks for context continuity
)
docs = text_splitter.split_documents(documents)
print(f"Total chunks created: {len(docs)}")
print(docs[0])

# 6. Create embeddings & build vector DB (Chroma)
embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
vectorstore = Chroma.from_documents(docs, embeddings)

# 7. Define retriever
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# 8. Initialize LLM (OpenAI)
llm = ChatOpenAI(model="gpt-4o-mini", openai_api_key=OPENAI_API_KEY)

# 9. Build RAG chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True
)

# 10. Run query
# query = "Summarize the main concepts explained in this document."
query = "임용절차는 무엇인가요?"
result = qa_chain.invoke({"query": query})

# 11. Print results
print("Answer:", result["result"])
print("Number of source documents:", len(result["source_documents"]))
for idx, doc in enumerate(result["source_documents"], 1):
    print(f"\n[{idx}] {doc.page_content[:300]}...\n")