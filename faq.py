# -*- coding: utf-8 -*-
"""FAQ.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fGAe5bGzFHtD_ziuWSoeMdyVMnKO2kfG
"""

!pip install openai chromadb langchain langchain-openai langchain-community

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.prompts import PromptTemplate

# 1. OpenAI LLM setup

from google.colab import userdata

OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.2, api_key=OPENAI_API_KEY)

# 2. FAQ document data (sample)
docs = [
    "Q: How do I submit an approval request? A: In the ERP system, go to the [Approval] menu and create a request form.",
    "Q: What is the procedure for requesting vacation? A: In the groupware [Attendance Management] menu, fill out a vacation request form and get approval from your supervisor.",
    "Q: Who should I contact for HR-related inquiries? A: You can contact the HR manager in the HR team by email."
]

# 3. Embedding & vector store creation
embeddings = OpenAIEmbeddings(model="text-embedding-3-small",
                              api_key=OPENAI_API_KEY)
vector_db = Chroma.from_texts(docs, embeddings)

# 4. Define user query
user_query = "I want to take a vacation, how can I apply?"

# 5. RAG-style search
related_doc = vector_db.similarity_search(user_query, k=1)[0].page_content
related_doc

# 6. Prompt creation
prompt_template = PromptTemplate.from_template("""
Refer to the following document and answer the user's question naturally.
Document: {context}
Question: {question}
""")
filled_prompt = prompt_template.format(context=related_doc, question=user_query)

# 7. Generate LLM response
response = llm.invoke(filled_prompt)

print("Retrieved Document:", related_doc)
print("Generated Answer:", response.content)

filled_prompt